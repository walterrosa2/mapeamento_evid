import re
import json
from loguru import logger
from src.leitor_txt import carregar_blocos
from src.gemini_api import enviar_bloco_para_gemini
from src.planilha import inicializar_planilha, adicionar_linha_excel
from config import ARQUIVO_PADRAO_TXT


# ============================================================
#  FUNÃ‡Ã•ES DE LIMPEZA E NORMALIZAÃ‡ÃƒO
# ============================================================

def limpar_texto_bruto(valor: str) -> str:
    """Remove quebras de linha, mÃºltiplos espaÃ§os e caracteres estranhos."""
    if not isinstance(valor, str):
        return ""
    valor = re.sub(r"[\r\n\t]+", " ", valor)
    valor = re.sub(r"\s{2,}", " ", valor)
    return valor.strip()


def normalizar_chaves(lista_dados: list) -> list:
    """Padroniza nomes de colunas para evitar duplicaÃ§Ãµes e inconsistÃªncias."""
    mapa_equivalencias = {
        "trecho": "Trecho",
        "trecho/pÃ¡gina": "Trecho",
        "trecho / pÃ¡gina": "Trecho",
        "pagina": "ReferÃªncia",
        "pÃ¡gina": "ReferÃªncia",
        "referencia": "ReferÃªncia",
        "referÃªncia": "ReferÃªncia",
        "conteudo": "ConteÃºdo",
        "conteÃºdo": "ConteÃºdo",
        "resumo": "Resumo",
        "tipo": "Tipo de EvidÃªncia",
        "tipo de evidÃªncia": "Tipo de EvidÃªncia",
        "tipo de evidencia": "Tipo de EvidÃªncia"
    }

    normalizados = []
    for item in lista_dados:
        novo = {}
        for chave, valor in item.items():
            chave_norm = mapa_equivalencias.get(chave.strip().lower(), chave.strip())
            novo[chave_norm] = limpar_texto_bruto(valor)
        normalizados.append(novo)
    return normalizados


# ============================================================
#  FUNÃ‡ÃƒO DE PARSE ROBUSTA PARA TABELAS MARKDOWN
# ============================================================

def parse_markdown_tabela(texto: str) -> list:
    """Extrai dados de tabelas Markdown tolerando desalinhamentos e pipes extras."""
    linhas = [l.strip() for l in texto.splitlines() if l.strip()]
    dados, cabecalho = [], []

    for i, linha in enumerate(linhas):
        # Detecta linha divisÃ³ria (--- ou :---)
        if re.match(r"^\|?[\-\:\s\|]+$", linha):
            # Tenta pegar o cabeÃ§alho na linha anterior
            if i > 0:
                cabecalho = [c.strip(" :") for c in re.split(r"\s*\|\s*", linhas[i - 1]) if c.strip()]
            continue

        # Processa linhas de dados
        if cabecalho and linha.startswith("|"):
            colunas = [c.strip() for c in re.split(r"\s*\|\s*", linha) if c.strip()]
            # Corrige desalinhamentos
            if len(colunas) < len(cabecalho):
                colunas += [""] * (len(cabecalho) - len(colunas))
            elif len(colunas) > len(cabecalho):
                colunas = colunas[:len(cabecalho)]
            dados.append(dict(zip(cabecalho, colunas)))

    return dados


# ============================================================
#  FUNÃ‡ÃƒO HÃBRIDA DE EXTRAÃ‡ÃƒO (JSON + MARKDOWN)
# ============================================================

def extrair_campos(texto: str) -> list:
    """
    Tenta interpretar a resposta da IA:
    1) JSON vÃ¡lido
    2) Tabela Markdown
    Retorna lista de dicionÃ¡rios com colunas padronizadas.
    """
    texto = texto.strip()

    # Tentativa 1 â€” JSON
    try:
        # Remover blocos de cÃ³digo markdown se presentes
        json_clean = re.sub(r"```json\s*|\s*```", "", texto).strip()
        data = json.loads(json_clean)
        if isinstance(data, list):
            return normalizar_chaves(data)
    except json.JSONDecodeError:
        pass

    # Tentativa 2 â€” Markdown
    dados_md = parse_markdown_tabela(texto)
    if dados_md:
        return normalizar_chaves(dados_md)

    return []


# ============================================================
#  FUNÃ‡ÃƒO DE LIMPEZA FINAL DE LINHAS
# ============================================================

def limpar_linha_vazia(evidencia: dict) -> dict:
    """Remove chaves com valores vazios antes de salvar."""
    return {k: v for k, v in evidencia.items() if v not in ("", None, "null")}


# ============================================================
#  PIPELINE PRINCIPAL DE PROCESSAMENTO
# ============================================================

def processar_todos_os_blocos():
    """
    Pipeline completo:
    1) Divide o arquivo em blocos
    2) Envia cada bloco Ã  Gemini
    3) Extrai e grava evidÃªncias normalizadas no Excel
    """
    logger.info("ğŸ“š Carregando blocos do arquivo TXT...")
    blocos = carregar_blocos(ARQUIVO_PADRAO_TXT)
    total_blocos = len(blocos)

    logger.info("ğŸ“„ Inicializando planilha...")
    inicializar_planilha()

    for i, bloco in enumerate(blocos):
        logger.info(f"ğŸš€ Processando bloco {i+1}/{total_blocos}...")

        resposta = enviar_bloco_para_gemini(bloco, bloco_id=i)
        if not resposta:
            logger.error(f"Erro: nenhum retorno recebido da Gemini para o bloco {i}.")
            continue

        evidencias = extrair_campos(resposta)
        if not evidencias:
            logger.warning(f"Nenhum dado extraÃ­do do bloco {i}. Verifique o retorno.")
            continue

        linhas_validas = 0
        for evidencia in evidencias:
            evidencia_limpa = limpar_linha_vazia(evidencia)
            if evidencia_limpa:
                adicionar_linha_excel(evidencia_limpa)
                linhas_validas += 1

        logger.success(f"âœ… {linhas_validas} evidÃªncia(s) vÃ¡lidas salva(s) na planilha.")

    logger.info("ğŸ Processamento finalizado com sucesso.")
